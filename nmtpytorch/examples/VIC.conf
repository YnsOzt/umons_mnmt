# Sample configuration file

###################################
# Note about variable intrapolation
###################################
# A variable "var" in a "section" can be referenced as ${var} from the
# same section and as ${section:var} from other sections.

###################################################
# Arguments common to training and mainloop.
# The defaults are defined in nmtpytorch/config.py
###################################################

[train]
# auto_N: Select and lock N free GPUs
#      N: Use Nth GPU
#  0,1,N: Use GPUs numbered 0,1 and N
# (Multi-GPU options are there but not implemented yet in
#  training logic.)
device_id: auto_1

# Print info to screen and log file each `disp_freq` minibatches
disp_freq: 30

# If > 0, the seed will be fixed for reproducibility
seed: 0

# A `model_type` should be the class name of the model you'd like to train
# See nmtpytorch/models/
model_type: VIC

# After this many validations without improvement, the training will stop.
patience: 20

# The training will stop after this many epochs
max_epochs: 100

# Same as above but in terms of mini-batch count
max_iterations: 1000000

# An evaluation on held-out `val_set` will be performed
# after each `eval_freq` minibatches
eval_freq: 0

# Evaluate on validation set once before starting training
eval_zero: False

# Validation warmup. No periodic evaluation
# will be performed before epoch `eval_start` is reached.
eval_start: 1

# One or many evaluation metrics for held-out evaluation
# Early-stopping criterion is always the first one
eval_metrics: bleu, meteor, loss

# Post-processing filters to apply to beam-search outputs and references
# in order to correctly compute metrics (check: nmtpytorch/filters.py)
eval_filters: de-bpe

# Beam size during evaluation
eval_beam: 12

# Batch size for batched beam-search on GPU
eval_batch_size: 12

# Save the best model w.r.t each metric provided in `eval_metrics`
save_best_metrics: True

# Saves a snapshot every `checkpoint_freq` minibatches
checkpoint_freq: 0

# Keeps a rolling buffer of `n_checkpoints` for periodic checkpointing
n_checkpoints: 4

# Scaling factor for L2 regularization
l2_reg: 1e-5

# Gradient clipping norm
gclip: 1

# Optimizers from PyTorch (in lowercase)
optimizer: adam


# Initial learning rate for the above optimizer (0 uses PyTorch' defaults)
lr: 0.0004
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2


# Training batch_size. Same is used for evaluation loss batching.
batch_size: 32

# Where to save the models
save_path: ./models

# If given and TensorbardX is installed, TensorBoard files will be
# stored under this folder.
tensorboard_dir: ${save_path}

##################################################################
# Below section is completely depend.ent on the model_type selected
# The defaults for these arguments are defined in the relevant
# nmtpytorch/models/<model.py>
##################################################################

[model]


# type of attention: mlp/dot (dot is untested)
att_type: mlp


dec_dim: 320
emb_dim: 200

n_channels: 1024

l2_norm: False

# CGRU decoder initialization
#   mean_ctx: h_0 = W_decinit.dot(tanh(mean(encoder states)))
#       zero: h_0 = 0
dec_init: mean_ctx


# 2-way: Shares input/output embeddings of decoder i.e. target-side
# 3-way: 2-way + source embedding sharing
#        this requires **same** vocabularies
#        Check -s argument of nmtpy-build-vocab
# False: disabled.
tied_emb: 2way



# Trains an NMT from en->de
# This automatically take cares of determining src->trg order for
# the below data files. You can just change this to de->en to train
# another NMT for the inverse direction.
direction: image:Numpy -> en:Text

#############################
# Here we define the datasets
#############################
[data]
# A placeholder for data root
root: ../../IC/data

# For the `*_set` named variables, here are the general rules:
#   The variables should always have the `_set` suffix.
#   The keys are language IDs and the values are filenames.
#   The values can be globs like *.en for models/datasets which support
#     multiple data files.
#   More than 2 languages/keys can be given. Only keys defined above
#   in the [model]'s `direction` variable will be considered.

# This variable should always be named as **train_set**
train_set: {'image': '${root}/videos/pre_extracted/train_reshaped.npy',
            'en': '${root}/texts/bpe10k/train.lc.norm.tok.bpe.en'}

# The held-out test set on which early-stopping evaluations
# will be performed.
# NOTE that the variable should always be named as **val_set**
val_set: {'image': '${root}/videos/pre_extracted/val_reshaped.npy',
            'en': '${root}/texts/bpe10k/val.lc.norm.tok.bpe.en'}

###############################################
# Vocabulary files created by nmtpy-build-vocab
# one per each language key
###############################################
[vocabulary]
en: ${data:root}/texts/bpe10k/train.lc.norm.tok.bpe.vocab.en
