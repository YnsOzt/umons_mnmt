[model]

#visual features
img_sequence: True
n_channels: 2048

# Config enc
use_sa_x: False
use_sa_y: True
use_sga: False
use_attflat: True

# Config RNN
enc_type: LSTM
att_type: mlp
lstm_num_layers: 1
enc_dim: 320
dec_dim: 320
emb_dim: 200
l2_norm: False
bidirectional: False

# Config Transformer
ff_dim: 640 #ff_dim
dropout_sa: 0.0 # dropout sa
trans_num_layers: 6 #nombre de layer
n_head: 4 #nombre de tÃªte du multihead attention
flat_mlp_size: 320

# Simple dropout after source embeddings
dropout_emb: 0.3
# Simple dropout top source encoder states
dropout_ctx: 0.5


# Config Dec
dec_init: mean_ctx
decoder_type: normal
tied_emb: 2way
dropout_out: 0.5
max_trg_len: 80
direction: en:Text, image:Numpy -> de:Text


[train]
# auto_N: Select and lock N free GPUs
#      N: Use Nth GPU
#  0,1,N: Use GPUs numbered 0,1 and N
# (Multi-GPU options are there but not implemented yet in
#  training logic.)
device_id: auto_1

# Print info to screen and log file each `disp_freq` minibatches
disp_freq: 30

# If > 0, the seed will be fixed for reproducibility
seed: 0

# A `model_type` should be the class name of the model you'd like to train
# See nmtpytorch/models/
model_type: IMT

# After this many validations without improvement, the training will stop.
patience: 20

# The training will stop after this many epochs
max_epochs: 100

# Same as above but in terms of mini-batch count
max_iterations: 1000000

# An evaluation on held-out `val_set` will be performed
# after each `eval_freq` minibatches
eval_freq: 0

# Evaluate on validation set once before starting training
eval_zero: False

# Validation warmup. No periodic evaluation
# will be performed before epoch `eval_start` is reached.
eval_start: 1

# One or many evaluation metrics for held-out evaluation
# Early-stopping criterion is always the first one
eval_metrics: bleu, meteor, loss

# Post-processing filters to apply to beam-search outputs and references
# in order to correctly compute metrics (check: nmtpytorch/filters.py)
eval_filters: de-bpe

# Beam size during evaluation
eval_beam: 12

# Batch size for batched beam-search on GPU
eval_batch_size: 12

# Save the best model w.r.t each metric provided in `eval_metrics`
save_best_metrics: True

# Saves a snapshot every `checkpoint_freq` minibatches
checkpoint_freq: 0

# Keeps a rolling buffer of `n_checkpoints` for periodic checkpointing
n_checkpoints: 4

# Scaling factor for L2 regularization
l2_reg: 1e-5

# Gradient clipping norm
gclip: 1

# Optimizers from PyTorch (in lowercase)
optimizer: adam


# Initial learning rate for the above optimizer (0 uses PyTorch' defaults)
lr: 0.0004
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2


# Training batch_size. Same is used for evaluation loss batching.
batch_size: 32

# Where to save the models
save_path: ./models

# If given and TensorbardX is installed, TensorBoard files will be
# stored under this folder.
tensorboard_dir: ${save_path}

#############################
# Here we define the datasets
#############################
[data]
# A placeholder for data root
root: data/IMT


train_set: {'en': '${root}/data.tok.bpe/train.lc.norm.tok.bpe.en',
            'image': '${root}/image_splits/train/flickr30k_train_transposed_10feats.npy',
            'de': '${root}/data.tok.bpe/train.lc.norm.tok.bpe.de'}


val_set: {'en': '${root}/data.tok.bpe/val.lc.norm.tok.bpe.en',
          'image': '${root}/image_splits/val/flickr30k_validation_transposed_10feats.npy',
          'de': '${root}/data.tok.bpe/val.lc.norm.tok.bpe.de'}


#   nmtpy translate -s test_2016_flickr,test_2017_flickr,val -o output
test_2017_flickr_set: {'en': '${root}/data.tok.bpe/test_2017_flickr.lc.norm.tok.bpe.en',
                       'image': '${root}/image_splits/test_2017_flickr/flickr2017_test_transposed_10feats.npy',
                       'de': '${root}/data.tok.bpe/test_2017_flickr.lc.norm.tok.bpe.de'}

test_2017_mscoco_set: {'en': '${root}/data.tok.bpe/test_2017_mscoco.lc.norm.tok.bpe.en',
                       'image': '${root}/image_splits/test_2017_coco/mscoco2017_test_transposed_10feats.npy',
                       'de': '${root}/data.tok.bpe/test_2017_mscoco.lc.norm.tok.bpe.de'}

test_2016_flickr_set: {'en': '${root}/data.tok.bpe/test_2016_flickr.lc.norm.tok.bpe.en',
                       'image': '${root}/image_splits/test_2016_flickr/flickr2016_test_transposed_10feats.npy',
                       'de': '${root}/data.tok.bpe/test_2016_flickr.lc.norm.tok.bpe.de'}

###############################################
# Vocabulary files created by nmtpy-build-vocab
# one per each language key
###############################################
[vocabulary]
en: ${data:root}/data.tok.bpe/train.lc.norm.tok.bpe.vocab.en
de: ${data:root}/data.tok.bpe/train.lc.norm.tok.bpe.vocab.de
